{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Read config from config file\n",
    "from pyspark.sql import SparkSession\n",
    "\n",
    "from pyspark.sql.functions import regexp_replace, split, udf\n",
    "from pyspark.sql.types import StructType, StructField, StringType, IntegerType, FloatType\n",
    "\n",
    "\n",
    "# Create SparkSession\n",
    "spark = SparkSession.builder.appName(\"Stoploss\").getOrCreate()\n",
    "\n",
    "configs = spark.read.option(\"header\", \"true\").csv(\"./data/StoplossConfigs.csv\")\n",
    "\n",
    "splitted_configs = configs.withColumn(\"site_name\", regexp_replace(configs.site_name, '_AT:1', ''))\\\n",
    "    .withColumn(\"site_name\", regexp_replace(\"site_name\", '_AT1', ''))\\\n",
    "        .withColumn(\"parts\", split(\"site_name\", \"`!`!`\"))\n",
    "\n",
    "# Add 4 new columns with this logic if any part is all numbers then it is adtag\n",
    "# if a single letter then country\n",
    "# if a string with . then domain\n",
    "# else customer\n",
    "def get_adtag(parts):\n",
    "    for part in parts:\n",
    "        if part.isdigit():\n",
    "            return part\n",
    "    return ''\n",
    "\n",
    "def get_country(parts):\n",
    "    for part in parts:\n",
    "        if len(part) == 1 and part.isalpha():\n",
    "            return part\n",
    "    return 'L'\n",
    "\n",
    "def get_domain(parts):\n",
    "    for part in parts:\n",
    "        if '.' in part:\n",
    "            return part\n",
    "    return ''\n",
    "\n",
    "def get_customer(parts):\n",
    "    for part in parts:\n",
    "        if not part.isdigit() and not (len(part) == 1 and part.isalpha()) and '.' not in part:\n",
    "            return part\n",
    "    return ''\n",
    "\n",
    "# Register UDFs\n",
    "udf_get_adtag = udf(get_adtag, StringType())\n",
    "udf_get_country = udf(get_country, StringType())\n",
    "udf_get_domain = udf(get_domain, StringType())\n",
    "udf_get_customer = udf(get_customer, StringType())\n",
    "# Add new columns using UDFs\n",
    "final_configs = splitted_configs.withColumn(\"adtag\", udf_get_adtag(\"parts\"))\\\n",
    "    .withColumn(\"country\", udf_get_country(\"parts\"))\\\n",
    "    .withColumn(\"domain\", udf_get_domain(\"parts\"))\\\n",
    "    .withColumn(\"customer\", udf_get_customer(\"parts\"))\\\n",
    "    .drop(\"parts\")\n",
    "\n",
    "# Show the result\n",
    "final_configs.show(truncate=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "adtags = spark.read.option(\"header\", \"true\").csv(\"./data/Customer and Ad Tag List (Alpha + Beta) - Ad tag.csv\")\n",
    "adtags.show()\n",
    "customers = spark.read.option(\"header\", \"true\").csv(\"./data/Customer and Ad Tag List (Alpha + Beta) - Customer.csv\")\n",
    "\n",
    "adtag_matchings = final_configs.join(adtags, final_configs.adtag == adtags.AdTagId, \"right\")\n",
    "adtag_matchings.repartition(1).write.option(\"header\", \"true\").csv(\"./data/adtag_matchings\")\n",
    "customers_matchings = final_configs.join(customers, final_configs.customer == customers.CMId, \"right\")\n",
    "customers_matchings.repartition(1).write.option(\"header\", \"true\").csv(\"./data/customers_matchings\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "devenv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
